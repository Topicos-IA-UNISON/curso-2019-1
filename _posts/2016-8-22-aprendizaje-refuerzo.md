---
layout: post
title: Arendizaje por refuerzo
description: Aprendizaje por refuerzo, de los métodos tabulares al aperendizaje profundo
image: assets/images/parte3.jpg
---

## Libro de texto

[Reinforcement Learning: An
Introduction](https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view)
de Richard S. Sutton and Andrew G. Barto. La biblia del aprendizaje por refuerzo (y más
esta segunda edición que publicará el MIT Press en noviembre de 2018). Publicado bajo un
acuerdo de *open access*, por lo que el libro está disponible en forma gratuita por los autores.


1. Los capítulos 3 al 5 incluyen los conceptos básicos de programación dinámica
2. El capítulo 6 y el capítulo 9 son esenciales para los conceptos básicos de RL.

## Material adicional

1. [**Introduction to Stochastic Dynamic
Programming**](http://www.deeplearningitalia.com/wp-content/uploads/2018/03/Introduction-to-Stochastic-Dynamic-Programming-Ross.pdf)
Sheldon Ross, Academic Press, 1983. El capítulo 2 presenta las ideas de los
métodos por descuento tal como estudian el tema en Matematicas. Un punto de
vista complementario para entender la notación usual.


2. [Demostración de optimalidad del método de iteración de
   políticas](http://ee266.stanford.edu/lectures/dpproof.pdf)


## Ejercicios a realizar

Programar en golang los siguientes ejemplos y ejercicios del libro de *Sutton y Barto*:

1. Programación dinámica

   a. Iteración de políticas: 

      - Ejemplo 4.2: *Jack's Car Rental*
      
      - Ejercicio 4.7: Variación del problema *Jack's Car Rental**

   b. Iteración de valores:
   
      - Ejemplo 4.3: El problema del jugador
      
      - Ejercicio 4.8: Discute los resultados de la política obtenida en el ejercicio anterior
      
      - Ejercicio 4.9: Variaciones del problema del jugador
       
2. Aprendizaje por refuerzo: métodos tabulares

   a. Algoritmo SARSA:
   
      - Ejemplo 6.5: *Windy Gridworld*
      
      - Ejercicio 6.9: *Windy Gridworld* con movimiento de rey
      
      - Ejercicio 6.10: *Windy Gridworld* estocástico
      
   b. Algoritmo Q-Learning (QL)
   
      - Ejemplo 6.6: *Cliff Walking*
      
      - Ejercicio 6.12: Pregunta interesante para ver con el problema *Cliff Walking*
      
3. Aprendizaje por refuerzo: métodos aproximados



## Proyecto para RL Tabular y/o *coarse coding*

## Proyecto para RL con redes profundas
